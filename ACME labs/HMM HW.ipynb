{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem 1:\n",
      "Part a:\n",
      "Probability of  (0, 0, 0) : 0.0\n",
      "Probability of  (0, 0, 1) : 0.0\n",
      "Probability of  (0, 1, 0) : 0.0\n",
      "Probability of  (0, 1, 1) : 0.0\n",
      "Probability of  (1, 0, 0) : 0.0028\n",
      "Probability of  (1, 0, 1) : 0.00024\n",
      "Probability of  (1, 1, 0) : 0.0168\n",
      "Probability of  (1, 1, 1) : 0.00504\n",
      "Total probability for  [ 1.  0.  2.] :  0.02488\n",
      "\n",
      "Part b:\n",
      "Alpha:\n",
      "[[ 0.          0.08695652  0.78778135]\n",
      " [ 1.          0.91304348  0.21221865]]\n",
      "\n",
      "Part c:\n",
      "I implemented the direct computation of P(O|lambda) and the alpha pass as algorithms, exactly like it said, and they worked.\n",
      "\n",
      "Part d:\n",
      "For part (a), we have five multiplications for each of the eight possible sequences, so the work factor is 40\n",
      "In terms of N and T, we have 2^N possible sequences with 2T-1 multiplications each, so work factor is 2^N*(2T-1)\n",
      "For part (b), we have fourteen multiplications, so the work factor is 14.\n",
      "In terms of N and T, we have N*(N+1) multiplications per step, except the first step is only N, so total of N+(T-1)*N*(N+1)\n",
      "We did not account for normalization in any of these calculations.\n",
      "\n",
      "Problem 2:\n",
      "The sequence (1,1,0) (CCH) has the highest probability of producing the observation sequence, so it is the best in the dynamic programming sense.\n",
      "The sequence (1,1,0) (CCH) also has the highest probability of being correct at each step, according to the alpha pass, so it is also the best in the HMM sense.\n",
      "\n",
      "Problem 3:\n",
      "Total probability over all observations:  1.0\n",
      "Using forward algorithm:\n",
      "[[ 48.83823529  47.85062196  47.66749346  47.6330901 ]\n",
      " [ 32.16176471  33.14937804  33.33250654  33.3669099 ]] 81.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel\\__main__.py:24: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "C:\\Anaconda\\lib\\site-packages\\ipykernel\\__main__.py:26: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "C:\\Anaconda\\lib\\site-packages\\ipykernel\\__main__.py:40: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "C:\\Anaconda\\lib\\site-packages\\ipykernel\\__main__.py:43: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import itertools as it\n",
    "\n",
    "def naive(A,B,pi,obs,print_steps=True):\n",
    "    \"\"\"\n",
    "    B should be column stochastic\n",
    "    obs contains the indices of the thing\n",
    "    \"\"\"\n",
    "    T = len(obs)\n",
    "    N = A.shape[0]\n",
    "    M = B.shape[0]\n",
    "    \n",
    "    tot = 0\n",
    "    for seq in it.product(range(N),repeat=T):\n",
    "        # seq is a sequence of length T\n",
    "        \n",
    "        prob = pi[seq[0]]\n",
    "        for i in xrange(T-1):\n",
    "            # Multiply by the probability of moving to the state in seq[i+1],\n",
    "            # given that you are in the state in seq[i]\n",
    "            prob *= A[seq[i],seq[i+1]]\n",
    "            # Get the probability of the i-th observation draw, given that \n",
    "            # you are in the state seq[i]\n",
    "            prob *= B[obs[i],seq[i]]\n",
    "        # Do the last beta thing\n",
    "        prob *= B[obs[T-1],seq[T-1]]\n",
    "        if print_steps:\n",
    "            print \"Probability of \", seq, \":\", prob\n",
    "        tot += prob\n",
    "    if print_steps:\n",
    "        print \"Total probability for \", obs, \": \", tot\n",
    "    return tot\n",
    "\n",
    "def alpha_pass(A,B,pi,obs,print_out=True):\n",
    "    T = len(obs) # of observations\n",
    "    N = A.shape[0] # of states\n",
    "    M = B.shape[0]\n",
    "    \n",
    "    alpha = np.zeros((N,T))\n",
    "    alpha[:,0] = pi*B[obs[0],:]\n",
    "    \n",
    "    for i in xrange(1,T):\n",
    "        alpha[:,i] = np.dot(A.T,alpha[:,i-1])*B[obs[i],:]\n",
    "    c = 1/np.sum(alpha,axis=0)\n",
    "    alpha = c*alpha\n",
    "    if print_out:\n",
    "        print \"Alpha:\\n\",alpha\n",
    "    return alpha, c\n",
    "\n",
    "def prob1():\n",
    "    print \"Problem 1:\"\n",
    "    print \"Part a:\"\n",
    "    A = np.array([[0.7,0.3],[0.4,0.6]])\n",
    "    B = np.array([[0.1,0.7],[0.4,0.2],[0.5,0.1]])\n",
    "    pi = np.array([0.0,1.0])\n",
    "    obs = np.array([1.0,0.0,2.0])\n",
    "    naive(A,B,pi,obs)\n",
    "    \n",
    "    print \"\\nPart b:\"\n",
    "    alpha_pass(A,B,pi,obs) \n",
    "    \n",
    "    print \"\\nPart c:\"\n",
    "    print \"I implemented the direct computation of P(O|lambda) and the alpha pass as algorithms, exactly like it said, and they worked.\"\n",
    "    \n",
    "    print \"\\nPart d:\"\n",
    "    print \"For part (a), we have five multiplications for each of the eight possible sequences, so the work factor is 40\"\n",
    "    print \"In terms of N and T, we have 2^N possible sequences with 2T-1 multiplications each, so work factor is 2^N*(2T-1)\"\n",
    "    print \"For part (b), we have fourteen multiplications, so the work factor is 14.\"\n",
    "    print \"In terms of N and T, we have N*(N+1) multiplications per step, except the first step is only N, so total of N+(T-1)*N*(N+1)\"\n",
    "    print \"We did not account for normalization in any of these calculations.\"\n",
    "    \n",
    "def prob2():\n",
    "    print \"\\nProblem 2:\"\n",
    "    print \"The sequence (1,1,0) (CCH) has the highest probability of producing the observation sequence, so it is the best in the dynamic programming sense.\"\n",
    "    print \"The sequence (1,1,0) (CCH) also has the highest probability of being correct at each step, according to the alpha pass, so it is also the best in the HMM sense.\"\n",
    " \n",
    "def prob3():\n",
    "    print \"\\nProblem 3:\"\n",
    "    tot = 0\n",
    "    A = np.array([[0.7,0.3],[0.4,0.6]])\n",
    "    B = np.array([[0.1,0.7],[0.4,0.2],[0.5,0.1]])\n",
    "    pi = np.array([0.6,0.4])\n",
    "    alpha = np.zeros((2,4))\n",
    "    for obs in it.product(range(3),repeat=4):\n",
    "        prob = naive(A,B,pi,obs,print_steps=False)\n",
    "        tot += prob\n",
    "        to_add, cT = alpha_pass(A,B,pi,obs,print_out=False)\n",
    "        alpha += to_add\n",
    "    print \"Total probability over all observations: \", tot\n",
    "    print \"Using forward algorithm:\\n\", alpha, np.sum(alpha[:,-1])\n",
    "    \n",
    "prob1()\n",
    "prob2()\n",
    "prob3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 4:\n",
    "\n",
    "Original: \n",
    "$$a_{ij} = \\sum_{t=0}^{T-2} \\gamma_t(i,j) \\bigg/ \\sum_{t=0}^{T-2} \\gamma_t(i)$$\n",
    "    \n",
    "$$b_j(k) = \\sum_{t\\in {0,1,\\dots,T-1}, \\mathcal{O}_t=k} \\gamma_t(j) \\bigg/ \\sum_{t=0}^{T-1} \\gamma_t(j)$$\n",
    "                \n",
    "$$\\gamma_t(i,j) = \\alpha_t(i)a_{ij}b_j(\\mathcal{O}_{t+1})\\beta_{t+1}(j)\\bigg/ P(\\mathcal{O}|\\lambda)$$\n",
    "\n",
    "Therefore, $a_{ij}$ and $b_j(k)$ in terms of $\\alpha$ and $\\beta$ (since the $P(\\mathcal{O}|\\lambda)$ cancels) are \n",
    "\n",
    "$$a_{ij} = \\sum_{t=0}^{T-2} \\alpha_t(i)a_{ij}b_j(\\mathcal{O}_{t+1})\\beta_{t+1}(j)\\bigg/ \\sum_{t=0}^{T-2} \\sum_{j=0}^{N-1} \\alpha_t(i)a_{ij}b_j(\\mathcal{O}_{t+1})\\beta_{t+1}(j)$$\n",
    "\n",
    "and\n",
    "\n",
    "$$b_j(k) = \\sum_{t\\in {0,1,\\dots,T-1}, \\mathcal{O}_t=k} \\sum_{i=0}^{N-1} \\alpha_t(j)a_{ji}b_i(\\mathcal{O}_{t+1})\\beta_{t+1}(i) \\bigg/ \\sum_{t=0}^{T-1} \\sum_{i=0}^{N-1} \\alpha_t(j)a_{ji}b_i(\\mathcal{O}_{t+1})\\beta_{t+1}(i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-8-62da3773c942>, line 29)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-62da3773c942>\"\u001b[1;36m, line \u001b[1;32m29\u001b[0m\n\u001b[1;33m    def reestimate():\u001b[0m\n\u001b[1;37m                     ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def beta_pass(A,B,pi,obs,c):\n",
    "    \"\"\"\n",
    "    A,B,pi are the guesses.\n",
    "    B gives you the betas. B is NxT. so beta_t(i) is the ith row, t-th column\n",
    "    c is a vector of length T\n",
    "    \"\"\"\n",
    "    T = len(obs) # of observations\n",
    "    N = A.shape[0] # of states\n",
    "    M = B.shape[0]\n",
    "    \n",
    "    beta = np.zeros_like(B)\n",
    "    beta[:,-1] = c\n",
    "    \n",
    "    for t in xrange(T-2,0,-1):\n",
    "        for i in xrange(N):\n",
    "            for j in xrange(N):\n",
    "                # beta_t(i) += a_ij*b_j(O_t+1)*beta_t+1(j)\n",
    "                beta[i,t] += A[i,j]*B[obs[t+1],j]*beta[j,t+1]\n",
    "            beta[i,t] *= c[t]\n",
    "    return beta\n",
    "                \n",
    "def gamma(A,B,alpha,beta):\n",
    "    # WHAT SHAPE IS GAMMA SUPPOSED TO BE\n",
    "    \n",
    "    for t in xrange(T-1):\n",
    "        denom = 0\n",
    "        for i in xrange(N):\n",
    "            for j in xrange(N):\n",
    "                denom += alpha[i,t]*A[i,j]*B[obs[t+1],j]*beta[j,t+1]\n",
    "        for i in xrange(N):\n",
    "            \n",
    "\n",
    "def reestimate():\n",
    "    pass\n",
    "\n",
    "\n",
    "def HMM(A,B,pi):\n",
    "    \"\"\"A, B, and pi are the initial guesses.\"\"\"\n",
    "    # The alpha pass (return cT)\n",
    "    \n",
    "    # The beta pass\n",
    "    \n",
    "    # Compute gammas\n",
    "    \n",
    "    # Re-estimate\n",
    "    \n",
    "    # Compute logprob\n",
    "    \n",
    "    # To continue iterating\n",
    "\n",
    "\n",
    "def prob3(data):\n",
    "    # Randomly initialize A, B and pi as given\n",
    "    pi = np.array([0.51316,0.48684])\n",
    "    A = np.array([[0.47468,0.52532],[0.51656,0.48344]])\n",
    "    B = 1/27.*np.ones((27,2)) + np.random.normal(loc=0,scale=0.003,size=(27,2))\n",
    "    \n",
    "    # Call HMM until done iterating\n",
    "    # len(obs) = T = 50k\n",
    "    # N = # of states = 2\n",
    "    # M = # of observation symbols = 27\n",
    "\n",
    "# Get the first 50k letters in a nice (' '=26, 'a'=0, etc.) format.\n",
    "with open('brown.txt','r') as myfile:\n",
    "    data = myfile.read()\n",
    "data = data[:50000]\n",
    "data = list(data)\n",
    "out = np.zeros(len(data))\n",
    "for i in xrange(len(data)):\n",
    "    if ord(data[i]) == 32:\n",
    "        out[i] = 26\n",
    "    else:\n",
    "        out[i] = ord(data[i]) - 97\n",
    "print 1/27.\n",
    "prob3(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
