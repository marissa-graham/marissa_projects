{
 "metadata": {
  "name": "",
  "signature": "sha256:e60b83851e84401e2284bcbdf2860f44804eee173ea59251c5ee0887f43378ec"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from bs4 import BeautifulSoup\n",
      "import re\n",
      "import urllib2\n",
      "from matplotlib import pyplot as plt\n",
      "import sqlite3 as sql\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.common.keys import Keys\n",
      "\n",
      "#Problem 1\n",
      "def prob1():\n",
      "    '''\n",
      "    Use the 'Big Bank Info' for the following:\n",
      "    Return a 2-d array which includes the Bank Name, Rank, ID, Domestic Assets, and number of Domestic Branches for each of JPMorgan, Capital One, and Discover'\n",
      "    '''\n",
      "    bank_soup = BeautifulSoup(open('Big-Bank-Info.htm'))\n",
      "    # Pull out Bank Name, Rank, ID, Domestic Assets, # of domestic branches\n",
      "    # 0,1,2,6,9\n",
      "    \n",
      "    col_names = ['Bank Name','Rank','ID','Domestic Assets','Domestic Branches']\n",
      "    table = bank_soup.find(name='table',rules=\"GROUPS\")\n",
      "    banks = table.find_all(name='tr')\n",
      "    jpmorgan = banks[1].contents\n",
      "    capitalone = banks[47].contents\n",
      "    discover = banks[53].contents\n",
      "    my_arr = []\n",
      "    n = 4\n",
      "    vals = [1,3,5,13,19]\n",
      "    for j in xrange(5):\n",
      "        i = vals[j]\n",
      "        newcol = [col_names[j],str(jpmorgan[i])[n:-n-1],str(capitalone[i])[n:-n-1],str(discover[i])[n:-n-1]]\n",
      "        my_arr.append(newcol)\n",
      "    print my_arr\n",
      "\n",
      "#Problem 2\n",
      "def prob2():\n",
      "    '''\n",
      "    Use the weather site listed in the lab to return the Actual Max Temperature located on the website.\n",
      "    Return the tag containing the 'Next Day' button.\n",
      "    Return the URL attached\n",
      "    '''\n",
      "    url = \"http://www.wunderground.com/history/airport/KSAN/2015/1/1/DailyHistory.html?req_city=San+Diego&req_state=CA&req_statename=California&reqdb.zip=92101&reqdb.magic=1&reqdb.wmo=99999&MR=1\"\n",
      "    content = urllib2.urlopen(url).read()\n",
      "    soup = BeautifulSoup(content)\n",
      "    tr = soup.find_all('tr')#,text=re.compile('Max'))\n",
      "    maximum = tr[3]\n",
      "    max_temp = maximum.find_all('span')[7].text\n",
      "    \n",
      "    return max_temp, soup.find(text=re.compile(\"Next Day\")).parent\n",
      "\n",
      "\n",
      "#Problem 3\n",
      "def prob3():\n",
      "    '''\n",
      "    Mimic the Wunderground Weather example to return an array of average temperatures of the year 2014 in San Diego.\n",
      "    Also draw a graph.\n",
      "    '''\n",
      "    # This takes forever to run\n",
      "    weather_url = \"https://www.wunderground.com/history/airport/KSAN/2014/1/1/DailyHistory.html\"\n",
      "    weather_content = urllib2.urlopen(weather_url).read()\n",
      "    weather_soup = BeautifulSoup(weather_content)\n",
      "    actual = []\n",
      "    while(\"2015\" not in weather_soup.find(class_=\"history-date\").string):\n",
      "        print weather_soup.find(class_=\"history-date\").string\n",
      "        while(len(weather_soup.find_all(string=\"Actual\")) != 1):\n",
      "            weather_content = urllib2.urlopen(weather_url).read()\n",
      "            weather_soup = BeautifulSoup(weather_content)\n",
      "        actual_temp = weather_soup.find(string=\"Max Temperature\").parent.parent.next_sibling.next_sibling.span.span.text\n",
      "        actual.append(int(actual_temp))\n",
      "        next_url = weather_soup.find(string=re.compile(\"Next Day\")).parent[\"href\"]\n",
      "        weather_url = \"https://www.wunderground.com\"+next_url\n",
      "        weather_content = urllib2.urlopen(weather_url).read()\n",
      "        weather_soup = BeautifulSoup(weather_content)\n",
      "\n",
      "#Problem 4\n",
      "def prob4():\n",
      "    '''\n",
      "    Choose one of three options\n",
      "    a) Google Finance data\n",
      "    b) Mens soccer stats\n",
      "    c) World Cup Women's soccer stats\n",
      "\n",
      "    create a SQL table with various information\n",
      "    '''\n",
      "\n",
      "    pass\n",
      "\n",
      "#Problem 5\n",
      "def prob5():\n",
      "    '''\n",
      "    Access NBA Stats website\n",
      "    Return list of 'a' tags for each NBA team containing URL\n",
      "    '''\n",
      "\n",
      "    pass\n",
      "\n",
      "#Problem 6\n",
      "def prob6():\n",
      "    '''\n",
      "    Using problem 5, create a SQL table containing each team name, win and loss totals\n",
      "    '''\n",
      "\n",
      "    pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}